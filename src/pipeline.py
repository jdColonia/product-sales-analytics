"""
Pipeline principal de anÃ¡lisis exploratorio de datos.

Este mÃ³dulo orquesta el flujo completo de anÃ¡lisis EDA,
gestionando la carga de datos, ejecuciÃ³n de anÃ¡lisis y
generaciÃ³n de reportes.
"""

import sys
import os

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from config.spark_config import create_spark_session, stop_spark_session
from src.data_loader import DataLoader
from src.eda_analyzer import EDAAnalyzer
from src.visualizer import DataVisualizer
from datetime import datetime


class SalesAnalyticsPipeline:
    """Pipeline principal para anÃ¡lisis de datos de ventas."""

    def __init__(self, app_name: str = "SalesAnalytics-EDA"):
        """
        Inicializa el pipeline.

        Args:
            app_name: Nombre de la aplicaciÃ³n Spark
        """
        self.app_name = app_name
        self.spark = None
        self.data_loader = None
        self.eda_analyzer = None
        self.visualizer = None
        self.start_time = None

    def initialize(self):
        """Inicializa los componentes del pipeline."""
        print(f"\n{'='*80}")
        print(f"ğŸš€ INICIANDO PIPELINE DE ANÃLISIS EXPLORATORIO DE DATOS")
        print(f"{'='*80}\n")

        self.start_time = datetime.now()

        print("ğŸ”§ Inicializando sesiÃ³n de Spark...")
        self.spark = create_spark_session(self.app_name)
        print("âœ… SesiÃ³n de Spark inicializada correctamente\n")

        self.data_loader = DataLoader(self.spark)
        self.eda_analyzer = EDAAnalyzer(self.spark)
        self.visualizer = DataVisualizer()

    def run_categories_analysis(self):
        """Ejecuta anÃ¡lisis del dataset de categorÃ­as."""
        print(f"\n{'#'*80}")
        print("ğŸ“‹ ANÃLISIS DEL DATASET: CATEGORÃAS DE PRODUCTOS")
        print(f"{'#'*80}")

        print("\nğŸ“‚ Cargando datos de categorÃ­as...")
        df_categories = self.data_loader.load_categories()
        df_categories.cache()
        print("âœ… Datos cargados correctamente\n")

        # Para tablas de dominio solo verificamos calidad de datos
        print("ğŸ“ NOTA: Este es un dataset de referencia (lookup table)")
        print("   Solo se verifica la estructura y calidad de datos\n")

        self.eda_analyzer.analyze_structure(df_categories, "categories")
        self.eda_analyzer.analyze_missing_values(df_categories, "categories")
        self.eda_analyzer.analyze_duplicates(df_categories, "categories")

        # Mostrar las categorÃ­as disponibles
        print("\nğŸ“‹ CategorÃ­as disponibles en el sistema:")
        print("-" * 60)
        df_categories.orderBy("category_id").show(50, truncate=False)

        df_categories.unpersist()

    def run_product_categories_analysis(self):
        """Ejecuta anÃ¡lisis del dataset de productos-categorÃ­as."""
        print(f"\n{'#'*80}")
        print("ğŸ“¦ ANÃLISIS DEL DATASET: RELACIÃ“N PRODUCTOS-CATEGORÃAS")
        print(f"{'#'*80}")

        print("\nğŸ“‚ Cargando datos de productos-categorÃ­as...")
        df_products = self.data_loader.load_product_categories()
        df_products.cache()
        print("âœ… Datos cargados correctamente\n")

        self.eda_analyzer.analyze_structure(df_products, "product_categories")
        self.eda_analyzer.analyze_missing_values(df_products, "product_categories")
        self.eda_analyzer.analyze_duplicates(
            df_products, "product_categories", subset=["product_id"]
        )

        # Los IDs (product_id, category_id) no tienen significado estadÃ­stico
        # Solo analizamos cardinalidad y distribuciÃ³n
        print("\nğŸ”¢ AnÃ¡lisis de cardinalidad de IDs:")
        print("-" * 60)
        print(
            f"   ğŸ“¦ Productos Ãºnicos: {df_products.select('product_id').distinct().count():,}"
        )
        print(
            f"   ğŸ“‹ CategorÃ­as Ãºnicas: {df_products.select('category_id').distinct().count():,}"
        )

        # AnÃ¡lisis de distribuciÃ³n: productos por categorÃ­a
        print("\nğŸ“Š DistribuciÃ³n de productos por categorÃ­a:")
        df_with_categories = df_products.join(
            self.data_loader.load_categories(), "category_id", "left"
        )

        from pyspark.sql.functions import count, desc

        products_per_category = (
            df_with_categories.groupBy("category_name")
            .agg(count("*").alias("num_productos"))
            .orderBy(desc("num_productos"))
        )

        print("\nTop 15 categorÃ­as con mÃ¡s productos:")
        products_per_category.show(15, truncate=False)

        # Generar visualizaciones
        print("\nğŸ“Š Generando visualizaciones...")
        self.visualizer.plot_category_distribution(
            df_with_categories, "product_categories", top_n=15
        )

        # GrÃ¡fica de productos por categorÃ­a
        self.visualizer.plot_top_items(
            df_with_categories,
            "category_name",
            "products_per_category",
            top_n=20,
            title="DistribuciÃ³n de Productos por CategorÃ­a",
            ylabel="CategorÃ­a",
        )

        df_products.unpersist()

    def run_transactions_analysis(self, sample_size: int = None):
        """
        Ejecuta anÃ¡lisis del dataset de transacciones.

        Args:
            sample_size: NÃºmero de filas a muestrear (None = todas)
        """
        print(f"\n{'#'*80}")
        print("ğŸ›’ ANÃLISIS DEL DATASET: TRANSACCIONES")
        print(f"{'#'*80}")

        stores = self.data_loader.get_available_stores()
        print(f"\nğŸª Tiendas disponibles: {', '.join(stores)}")

        print("\nğŸ“‚ Cargando datos de transacciones...")
        df_transactions = self.data_loader.load_transactions()

        if sample_size:
            print(f"âš ï¸ Tomando muestra de {sample_size:,} registros...")
            df_transactions = df_transactions.limit(sample_size)

        df_transactions.cache()
        print("âœ… Datos cargados correctamente\n")

        self.eda_analyzer.analyze_structure(df_transactions, "transactions")
        self.eda_analyzer.analyze_missing_values(df_transactions, "transactions")
        self.eda_analyzer.analyze_duplicates(df_transactions, "transactions")

        # Analizar cantidad de productos por transacciÃ³n
        from pyspark.sql.functions import size, split, trim, col
        print("\nğŸ“Š AnÃ¡lisis de productos por transacciÃ³n:")
        print("-" * 60)
        df_with_count = df_transactions.withColumn(
            "num_productos", size(split(trim(col("products")), " "))
        )
        self.eda_analyzer.analyze_numeric_columns(
            df_with_count, "transactions", numeric_columns=["num_productos"]
        )

        # Analizamos cardinalidad y patrones de uso
        print("\nğŸ”¢ AnÃ¡lisis de cardinalidad de IDs:")
        print("-" * 60)
        print(
            f"   ğŸª Tiendas Ãºnicas: {df_transactions.select('store_id').distinct().count():,}"
        )
        print(
            f"   ğŸ‘¥ Clientes Ãºnicos: {df_transactions.select('customer_id').distinct().count():,}"
        )
        print(f"   ğŸ›’ Transacciones totales: {df_transactions.count():,}")

        # AnÃ¡lisis de frecuencia de clientes (top clientes por nÃºmero de transacciones)
        print("\nğŸ† Top 10 clientes con mÃ¡s transacciones:")
        from pyspark.sql.functions import count, desc

        df_transactions.groupBy("customer_id").agg(
            count("*").alias("num_transacciones")
        ).orderBy(desc("num_transacciones")).show(10, truncate=False)

        self.eda_analyzer.analyze_categorical_columns(
            df_transactions,
            "transactions",
            categorical_columns=["transaction_date"],
            top_n=15,
        )

        # Generar visualizaciones
        print("\nğŸ“Š Generando visualizaciones...")
        self.visualizer.plot_top_items(
            df_transactions,
            "customer_id",
            "transactions",
            top_n=20,
            title="Top 20 Clientes con MÃ¡s Transacciones",
            ylabel="Customer ID",
        )
        self.visualizer.plot_temporal_trend(
            df_transactions, "transaction_date", "transactions", sample_dates=None
        )

        df_transactions.unpersist()

    def run_transactions_exploded_analysis(self, sample_size: int = None):
        """
        Ejecuta anÃ¡lisis de transacciones con productos explodidos.

        Args:
            sample_size: NÃºmero de transacciones a muestrear antes de explodir
        """
        print(f"\n{'#'*80}")
        print(
            "ğŸ¯ ANÃLISIS DEL DATASET: TRANSACCIONES EXPLODIDAS (DETALLE POR PRODUCTO)"
        )
        print(f"{'#'*80}")

        print("\nğŸ“‚ Cargando y procesando transacciones...")
        df_transactions = self.data_loader.load_transactions()

        if sample_size:
            print(f"âš ï¸ Tomando muestra de {sample_size:,} transacciones...")
            df_transactions = df_transactions.limit(sample_size)

        print("ğŸ”„ Explodiendo productos (un producto por fila)...")
        df_exploded = self.data_loader.explode_transactions(df_transactions)
        df_exploded.cache()
        print("âœ… Datos procesados correctamente\n")

        self.eda_analyzer.analyze_structure(df_exploded, "transactions_exploded")
        self.eda_analyzer.analyze_missing_values(df_exploded, "transactions_exploded")
        self.eda_analyzer.analyze_duplicates(df_exploded, "transactions_exploded")

        # AnÃ¡lisis de cardinalidad y frecuencias (no estadÃ­sticas descriptivas de IDs)
        print("\nğŸ”¢ AnÃ¡lisis de cardinalidad de IDs:")
        print("-" * 60)
        print(
            f"   ğŸ“¦ Productos Ãºnicos: {df_exploded.select('product_id').distinct().count():,}"
        )
        print(
            f"   ğŸ‘¥ Clientes Ãºnicos: {df_exploded.select('customer_id').distinct().count():,}"
        )
        print(
            f"   ğŸª Tiendas Ãºnicas: {df_exploded.select('store_id').distinct().count():,}"
        )
        print(f"   ğŸ›ï¸ Registros totales (productos vendidos): {df_exploded.count():,}")

        # Top productos mÃ¡s vendidos
        print("\nğŸ† Top 15 productos mÃ¡s vendidos:")
        from pyspark.sql.functions import count, desc

        df_exploded.groupBy("product_id").agg(
            count("*").alias("veces_vendido")
        ).orderBy(desc("veces_vendido")).show(15, truncate=False)

        self.eda_analyzer.analyze_categorical_columns(
            df_exploded,
            "transactions_exploded",
            categorical_columns=["transaction_date"],
            top_n=15,
        )

        # Generar visualizaciones
        print("\nğŸ“Š Generando visualizaciones...")
        self.visualizer.plot_top_items(
            df_exploded,
            "product_id",
            "transactions_exploded",
            top_n=20,
            title="Top 20 Productos MÃ¡s Vendidos",
            ylabel="Product ID",
        )
        self.visualizer.plot_temporal_trend(
            df_exploded, "transaction_date", "transactions_exploded", sample_dates=None
        )

        df_exploded.unpersist()

    def finalize(self):
        """Finaliza el pipeline."""
        print(f"\n{'='*80}")
        print("ğŸ FINALIZANDO PIPELINE")
        print(f"{'='*80}\n")

        self.eda_analyzer.generate_summary_report()

        # Crear resumen de visualizaciones generadas
        print("\nğŸ“Š Resumen de visualizaciones:")
        self.visualizer.create_summary_report()

        end_time = datetime.now()
        duration = end_time - self.start_time

        print(f"\nâ° Hora de finalizaciÃ³n: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"âŒ› Tiempo total de ejecuciÃ³n: {duration}")

        print("\nğŸ›‘ Deteniendo sesiÃ³n de Spark...")
        stop_spark_session(self.spark)
        print("âœ… SesiÃ³n de Spark detenida correctamente\n")

        print(f"{'='*80}")
        print("ğŸ‰ PIPELINE COMPLETADO EXITOSAMENTE")
        print(f"{'='*80}\n")
